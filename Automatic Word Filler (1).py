# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zM7gmpQCwp00FxmEvFXb-Gp1L_5TlDx1
"""

import yfinance as yf

import pandas as pd
import datetime
from datetime import date, timedelta
today = date.today()

d1 = today.strftime('%Y-%m-%d')
end_date = d1
d2 = date.today() - timedelta(days = 5000)
d2 = d2.strftime('%Y-%m-%d')
start_date = d2

data = yf.download('AAPL',
                   start = start_date,
                   end = end_date)

import numpy as np

from google.colab import files
path_to_file = list(files.upload().keys())[0]

# read the text file
with open(path_to_file, 'r', encoding='utf-8') as file:
  text = file.read()

import tensorflow as tf

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense

# now let's tokenize the text to create a sequence of words

tokenizer = Tokenizer()

tokenizer.fit_on_texts([text])

total_words = len(tokenizer.word_index) +1

# In the above code, the text is tokenized, which means it is divided into individual words or tokens.
# The Tokenizer' object is created, which will handle the tokenization process. The 'fit_on_texts' method of the tokenizer is called, passing the 'text' as input.
# This method analyzes the text and builds a vocabulary of unique words, assigning each word a numerical index.
# The 'total_words' variable is then assigned the value of the length of the word index plus one,
# representing the total number of distinct words in the text.

input_sequences = []
for line in text.split('\n'):
  token_list = tokenizer.texts_to_sequences([line])[0]
  for i in range(1, len(token_list)):
    n_gram_sequence = token_list[:i+1]
    input_sequences.append(n_gram_sequence)

# in the above code, the text data is split into lines using the '\n' character as a delimiter. For each line in the text, texts_to_sequences'
# method of the tokenizer is used to convert the line into a sequence of numerical tokens based on the previously created vocabulary.
# The resulting token list is then iterated over using a for loop. each iteration, a subsequence, or n-gram, of tokens is extracted, ranging from the beginning of the token list up to the current index 'I'.
# This n-gram sequence represents the input context, with the last token being the target or predicted word. This n-gram sequence is then appended to the 'input_sequences' list.
# This process is repeated for all lines in the text, generating multiple input-output sequences that will be used for training the next word prediction model.

max_sequence_len = max([len(seq) for seq in input_sequences])
input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))

# In the above code, the input sequences are padded to ensure all sequences have the same length.
# The variable max_sequence_len' is assigned the maximum length among all the input sequences. The 'pad_sequences'
# function is used to pad or truncate the input sequences to match this maximum length.


# The 'pad_sequences' function takes the input_sequences list, sets the maximum length to 'max_sequence_len',
# and specifies that the padding should be added at the beginning of each sequence using the 'padding=pre'
# argument.
# Finally,Now the input sequences are converted into a numpy array to facilitate further processing.
# let's split the sequences into input and output:

x = input_sequences[:, :-1]
y = input_sequences[:, -1]

y = np.array(tf.keras.utils.to_categorical(y, num_classes=total_words))

# in the above code , we are converting the ouput array into a suitable format for training a model, where each
# target word is represented as a binary

# now let's build a neural architecture to train the model:

model = Sequential()
model.add(Embedding(total_words, 100, input_length= max_sequence_len-1))
model.add(LSTM(150))
model.add(Dense(total_words, activation = 'softmax'))
print(model.summary())

# 'total_number' which represent the total number of distinct words in the vocabulary;
# (2). '100', which denotes the dimensionality of the word embeddings;

#  (3). and input_length', which specifies the length of the input sequences.

# The next layer added is the 'LSTM' layer, a type of recurrent neural network (RNN) layer designed for
# capturing dependencies in the data. It has 150 units, which means it will learn 150 internal representations or memory cells

# Finally, the 'Dense' layer is added, which is a fully connected layer that produces the output predictions.
# It has 'total_words' units and uses the 'softmax' activation function to convert the predicted scores into probabilities,
# indicating the likelihood of each word being the next one in the sequence.

# Now let's compile and train the model:

model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics =['accuracy'])
model.fit(x,y, epochs = 100, verbose =1)

# seed_text = 'I will leave if they

seed_text = "are you"
next_word = 3

for _ in range(next_words):
  token_list = tokenizer.texts_to_sequences([seed_text])[0]
  token_list = pad_sequences([token_list], maxlen = max_sequence_len-1, padding='pre')
  predicted = np.argmax(model.predict(token_list), axis =-1)

  output_word =""

  for word, index in tokenizer.word_index.items():
    if index == predicted:
      output_word = word
      break
    seed_text += " " + oupput_word

  print(seed_text)

# The above code generates the next word predictions based on a given seed text. The 'seed_text' variable holds
# the initial text. The 'next_words' variable determines the number of predictions to be generated. Inside the for
# loop, the 'seed_text' is converted into a sequence of tokens using the tokenizer. The token sequence is padded
# to match the maximum sequence length.


# The model predicts the next word by calling the 'predict' method on the model with the padded token sequence.
# The predicted word is obtained by finding the word with the highest probability score using 'np.argmax'. Then,
# the predicted word is appended to the 'seed_text', and the process is repeated for the desired number of 'next_words'.
# Finally, the 'seed_text' is printed, which contains the initial text followed by the generated predictions.

